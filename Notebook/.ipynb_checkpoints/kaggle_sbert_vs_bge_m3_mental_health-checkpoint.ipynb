{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9066eae1",
   "metadata": {},
   "source": [
    "# NLP Santé mentale – Baseline SBERT vs BGE‑M3 (Kaggle)\n",
    "\n",
    "Notebook prêt à exécuter sur Kaggle avec le dataset *Mental Health Text Classification Dataset (4‑Class)*.\n",
    "\n",
    "- Données : fichiers `mental_heath_unbanlanced.csv` (train) et `mental_health_combined_test.csv` (test)\n",
    "- Baseline : SBERT embeddings + régression logistique\n",
    "- Nouveau modèle : BGE‑M3 embeddings + régression logistique\n",
    "\n",
    "Remarque : le fichier `mental_heath_feature_engineered.csv` est optionnel et non utilisé ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances\n",
    "!pip -q install -U sentence-transformers scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5102dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efc05c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers disponibles dans le dossier dataset :\n",
      "['mental_health_combined_test.csv', 'mental_heath_unbanlanced.csv', 'mental_heath_feature_engineered.csv']\n"
     ]
    }
   ],
   "source": [
    "# Chemins des fichiers du dataset Kaggle\n",
    "# Adapter uniquement si le dossier Kaggle diffère\n",
    "base_dir = \"../data/\"\n",
    "\n",
    "train_path = os.path.join(base_dir, \"mental_heath_unbanlanced.csv\")\n",
    "test_path = os.path.join(base_dir, \"mental_health_combined_test.csv\")\n",
    "\n",
    "print(\"Fichiers disponibles dans le dossier dataset :\")\n",
    "print(os.listdir(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5205c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (49612, 3)\n",
      "Test : (992, 2)\n",
      "\n",
      "Colonnes train: ['Unique_ID', 'text', 'status']\n",
      "\n",
      "Répartition des classes (train):\n",
      "status\n",
      "Normal        18391\n",
      "Depression    14506\n",
      "Suicidal      11212\n",
      "Anxiety        5503\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "df_train = pd.read_csv(train_path).dropna(subset=[\"text\", \"status\"]).copy()\n",
    "df_test = pd.read_csv(test_path).dropna(subset=[\"text\", \"status\"]).copy()\n",
    "\n",
    "print(\"Train:\", df_train.shape)\n",
    "print(\"Test :\", df_test.shape)\n",
    "print(\"\\nColonnes train:\", list(df_train.columns))\n",
    "print(\"\\nRépartition des classes (train):\")\n",
    "print(df_train[\"status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479a7ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_len</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49612.000000</td>\n",
       "      <td>49612.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>399.227143</td>\n",
       "      <td>78.229199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>617.780591</td>\n",
       "      <td>122.187106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>242.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>923.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>1114.450000</td>\n",
       "      <td>220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>2345.560000</td>\n",
       "      <td>449.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>38785.000000</td>\n",
       "      <td>9684.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text_len    word_count\n",
       "count  49612.000000  49612.000000\n",
       "mean     399.227143     78.229199\n",
       "std      617.780591    122.187106\n",
       "min        7.000000      1.000000\n",
       "50%      242.000000     47.000000\n",
       "90%      923.000000    182.000000\n",
       "95%     1114.450000    220.000000\n",
       "99%     2345.560000    449.890000\n",
       "max    38785.000000   9684.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA minimal : longueurs de texte (caractères et mots)\n",
    "df_train[\"text_len\"] = df_train[\"text\"].astype(str).str.len()\n",
    "df_train[\"word_count\"] = df_train[\"text\"].astype(str).str.split().map(len)\n",
    "\n",
    "df_train[[\"text_len\", \"word_count\"]].describe(percentiles=[0.5, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3286c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anxiety', 'Depression', 'Normal', 'Suicidal']\n"
     ]
    }
   ],
   "source": [
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_train[\"status\"])\n",
    "y_test = label_encoder.transform(df_test[\"status\"])\n",
    "\n",
    "X = df_train[\"text\"].astype(str).tolist()\n",
    "X_test = df_test[\"text\"].astype(str).tolist()\n",
    "\n",
    "print(\"Classes:\", list(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207e4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 39689\n",
      "Val split  : 9923\n"
     ]
    }
   ],
   "source": [
    "# Split train / validation\n",
    "# Le jeu de test reste séparé et n'est pas utilisé pendant l'entraînement\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train split:\", len(X_tr))\n",
    "print(\"Val split  :\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883cdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache d'embeddings sur disque pour éviter de recalculer en cas de relance\n",
    "def embed_with_cache(model, texts, path, batch_size=32):\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path)\n",
    "    emb = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    np.save(path, emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a37d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement + évaluation d'un classifieur simple\n",
    "def train_and_eval(emb_tr, emb_val, emb_test, y_tr, y_val, y_test, class_names):\n",
    "    clf = LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\")\n",
    "    clf.fit(emb_tr, y_tr)\n",
    "\n",
    "    val_pred = clf.predict(emb_val)\n",
    "    test_pred = clf.predict(emb_test)\n",
    "\n",
    "    val_report = classification_report(y_val, val_pred, target_names=class_names, output_dict=True)\n",
    "    test_report = classification_report(y_test, test_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "    return clf, val_pred, test_pred, val_report, test_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493d28b",
   "metadata": {},
   "source": [
    "## Baseline : SBERT embeddings + régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54ddac-ff16-4416-8aa5-b840fca20d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ab619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle SBERT baseline (embeddings généralistes solides)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cpu\")\n",
    "#sbert = SentenceTransformer(sbert_name)\n",
    "\n",
    "# Option utile pour accélérer (perte potentielle de qualité)\n",
    "# sbert.max_seq_length = 256\n",
    "\n",
    "X_tr_sbert = embed_with_cache(sbert, X_tr, \"/kaggle/working/X_tr_sbert.npy\", batch_size=64)\n",
    "X_val_sbert = embed_with_cache(sbert, X_val, \"/kaggle/working/X_val_sbert.npy\", batch_size=64)\n",
    "X_test_sbert = embed_with_cache(sbert, X_test, \"/kaggle/working/X_test_sbert.npy\", batch_size=64)\n",
    "\n",
    "clf_sbert, val_pred_sbert, test_pred_sbert, val_rep_sbert, test_rep_sbert = train_and_eval(\n",
    "    X_tr_sbert, X_val_sbert, X_test_sbert, y_tr, y_val, y_test, label_encoder.classes_\n",
    ")\n",
    "\n",
    "print(\"SBERT - validation\")\n",
    "print(classification_report(y_val, val_pred_sbert, target_names=label_encoder.classes_))\n",
    "print(\"SBERT - test\")\n",
    "print(classification_report(y_test, test_pred_sbert, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion SBERT sur le test\n",
    "confusion_matrix(y_test, test_pred_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a79eab",
   "metadata": {},
   "source": [
    "## Nouveau modèle : BGE‑M3 embeddings + régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle BGE-M3 (embeddings récents)\n",
    "bge_name = \"BAAI/bge-m3\"\n",
    "bge = SentenceTransformer(bge_name)\n",
    "\n",
    "# Option utile si tu veux accélérer (réduit la longueur max traitée)\n",
    "# bge.max_seq_length = 256\n",
    "\n",
    "X_tr_bge = embed_with_cache(bge, X_tr, \"/kaggle/working/X_tr_bge.npy\", batch_size=32)\n",
    "X_val_bge = embed_with_cache(bge, X_val, \"/kaggle/working/X_val_bge.npy\", batch_size=32)\n",
    "X_test_bge = embed_with_cache(bge, X_test, \"/kaggle/working/X_test_bge.npy\", batch_size=32)\n",
    "\n",
    "clf_bge, val_pred_bge, test_pred_bge, val_rep_bge, test_rep_bge = train_and_eval(\n",
    "    X_tr_bge, X_val_bge, X_test_bge, y_tr, y_val, y_test, label_encoder.classes_\n",
    ")\n",
    "\n",
    "print(\"BGE-M3 - validation\")\n",
    "print(classification_report(y_val, val_pred_bge, target_names=label_encoder.classes_))\n",
    "print(\"BGE-M3 - test\")\n",
    "print(classification_report(y_test, test_pred_bge, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56820d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion BGE-M3 sur le test\n",
    "confusion_matrix(y_test, test_pred_bge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb428b",
   "metadata": {},
   "source": [
    "## Comparaison synthétique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison (macro F1 et accuracy)\n",
    "def extract_metrics(rep):\n",
    "    return {\n",
    "        \"accuracy\": rep[\"accuracy\"],\n",
    "        \"f1_macro\": rep[\"macro avg\"][\"f1-score\"],\n",
    "        \"precision_macro\": rep[\"macro avg\"][\"precision\"],\n",
    "        \"recall_macro\": rep[\"macro avg\"][\"recall\"],\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "rows.append({\"modele\": \"SBERT\", \"split\": \"validation\", **extract_metrics(val_rep_sbert)})\n",
    "rows.append({\"modele\": \"SBERT\", \"split\": \"test\", **extract_metrics(test_rep_sbert)})\n",
    "rows.append({\"modele\": \"BGE-M3\", \"split\": \"validation\", **extract_metrics(val_rep_bge)})\n",
    "rows.append({\"modele\": \"BGE-M3\", \"split\": \"test\", **extract_metrics(test_rep_bge)})\n",
    "\n",
    "df_compare = pd.DataFrame(rows)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dab1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des métriques (utile pour ton rapport)\n",
    "out_path = \"/kaggle/working/metrics_compare.csv\"\n",
    "df_compare.to_csv(out_path, index=False)\n",
    "out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bdf26",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Le jeu `mental_health_combined_test.csv` est réservé à l’évaluation finale.\n",
    "- Le fichier `mental_heath_feature_engineered.csv` est utile pour des baselines classiques, mais pas nécessaire pour SBERT/BGE.\n",
    "- Si la RAM GPU est limitée, diminue `batch_size` ou fixe `max_seq_length = 256`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
